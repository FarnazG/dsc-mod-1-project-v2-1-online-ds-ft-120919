{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I plan on loading in my data, structuring it and getting it ready for EDA, there will be no EDA in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # in case I need to do anything with path/dir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zippedData/tmdb.movies.csv',\n",
       " 'zippedData/imdb.title.crew.csv',\n",
       " 'zippedData/tn.movie_budgets.csv',\n",
       " 'zippedData/imdb.title.ratings.csv',\n",
       " 'zippedData/imdb.name.basics.csv',\n",
       " 'zippedData/imdb.title.principals.csv',\n",
       " 'zippedData/imdb.title.akas.csv',\n",
       " 'zippedData/bom.movie_gross.csv',\n",
       " 'zippedData/imdb.title.basics.csv']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use glob to get all of the csv files\n",
    "csv_files = glob(\"zippedData/*.csv\")\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with \n",
    "# keys as csv filenames (cleaned)\n",
    "# values as their respective dataframes\n",
    "csv_files_dict = {}\n",
    "for filename in csv_files:\n",
    "    filename_cleaned = os.path.basename(filename).replace(\".csv\", \"\").replace(\".\", \"_\")\n",
    "    filename_df = pd.read_csv(filename, index_col=0)\n",
    "    csv_files_dict[filename_cleaned] = filename_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files into sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will create a sqlite file in your directory\n",
    "conn = sqlite3.connect(\"movies_db.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this converts a dataframe to a sqlite table, with the ame 'tmdb_movies'\n",
    "# just let sql create the schema itself\n",
    "\n",
    "df_tmdb_movies.to_sql(\"tmdb_movies\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that converts the dataframe to a sqlite table\n",
    "def create_sql_table_from_df(df, name, conn):\n",
    "    # Use try except\n",
    "    # it will try to make a table\n",
    "    # if a table exists the function will execute whatever you put in the except part\n",
    "    try:\n",
    "        df.to_sql(name, conn)\n",
    "        print(f\"Created table {name}\")\n",
    "    \n",
    "    # if the table exists t will tell you, and won't cause an error\n",
    "    except Exception as e:\n",
    "        print(f\"could not make table {name}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the dictionary from earlier, that contains the keys and dataframes of all the files\n",
    "# We can create the tables programmatically\n",
    "for name, table in csv_files_dict.items():\n",
    "    create_sql_table_from_df(table, name, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a .gitignore file that will ignore the files that you unzipped and the sqlite file\n",
    "# look at mine for a reference\n",
    "# you cannot push files bigger than 100 MB to github from your computer\n",
    "\n",
    "# you can create your file here in jupyter and open it from jupyter\n",
    "with open(\"./.gitignore\", \"w+\") as f:\n",
    "    f.write(\"*.sqlite\") # put files you want to ignore here\n",
    "    f.write(\"\\n\") # insert a new line after each file\n",
    "    f.write(\"zippedData/\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"zippedData/*.csv\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"zippedData/*.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tmdb_movies',),\n",
       " ('imdb_title_crew',),\n",
       " ('tn_movie_budgets',),\n",
       " ('imdb_title_ratings',),\n",
       " ('imdb_name_basics',),\n",
       " ('imdb_title_principals',),\n",
       " ('imdb_title_akas',),\n",
       " ('bom_movie_gross',),\n",
       " ('imdb_title_basics',)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view all tables in db\n",
    "conn.execute(\"select name from sqlite_master where type='table';\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now continue doing all the EDA work in the EDA notebook (you have to make this yourself)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatiron-env] *",
   "language": "python",
   "name": "conda-env-flatiron-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
